{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP0PNAyDmT8i/qf+zjinUwy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"b7cAeFvqLd5A"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["**Q 1. Explain the core components of the Hadoop ecosystem and their respective roles in processing and storing big data. Provide a brief overview of HDFS, MapReduce, and YARN.**"],"metadata":{"id":"ivTb-zV0WV2R"}},{"cell_type":"markdown","source":["Here's an overview of the core components of the Hadoop ecosystem:\n","\n","Core Components of the Hadoop Ecosystem\n","\n","1. HDFS (Hadoop Distributed File System): A distributed storage system that stores data in a cluster of nodes. HDFS provides a scalable and fault-tolerant way to store large amounts of data.\n","\n","2. MapReduce: A programming model and framework for processing large datasets in parallel. MapReduce consists of two phases: Map and Reduce. The Map phase processes the data in parallel, while the Reduce phase aggregates the output from the Map phase.\n","\n","3. YARN (Yet Another Resource Negotiator): A resource management layer that manages resources and schedules applications. YARN provides a scalable and flexible way to manage resources in a Hadoop cluster.\n","\n","Brief Overview of Each Component\n","\n","1. HDFS: HDFS is a distributed storage system that stores data in a cluster of nodes. It provides a scalable and fault-tolerant way to store large amounts of data.\n","\n","2. MapReduce: MapReduce is a programming model and framework for processing large datasets in parallel. It consists of two phases: Map and Reduce. The Map phase processes the data in parallel, while the Reduce phase aggregates the output from the Map phase.\n","\n","3. YARN: YARN is a resource management layer that manages resources and schedules applications. It provides a scalable and flexible way to manage resources in a Hadoop cluster.\n","\n","By working together, these components provide a scalable and flexible way to process and store big data.\n"],"metadata":{"id":"0tgwVsO9MENN"}},{"cell_type":"markdown","source":["**Q.2 Discuss the Hadoop Distributed File System (HDFS) in detail. Explain how it stores and manages  data in a\n","distributed environment. Describe the key concepts of HDFS, such as NameNode, DataNode, and blocks, and\n","how they contribute to data reliability and fault tolerance.**\n"],"metadata":{"id":"krAtPwsQWjd1"}},{"cell_type":"markdown","source":["Overview of HDFS\n","\n","HDFS is a distributed storage system that stores data in a cluster of nodes. It is designed to store large amounts of data in a scalable and fault-tolerant manner. HDFS is the primary storage system used in Hadoop.\n","\n","Key Concepts of HDFS\n","\n","1. NameNode: The NameNode is the master node of the HDFS cluster. It maintains a directory hierarchy of the data stored in HDFS and tracks the location of data blocks on the DataNodes.\n","\n","2. DataNode: The DataNode is the slave node of the HDFS cluster. It stores the actual data blocks and serves read and write requests from clients.\n","\n","\n","3. Blocks: Blocks are the smallest unit of data storage in HDFS. Each block is typically 64MB or 128MB in size. Data is split into blocks and stored across multiple DataNodes for redundancy and fault tolerance.\n","\n","How HDFS Stores and Manages Data\n","\n","Here's how HDFS stores and manages data:\n","\n","1. Data Ingestion: Clients write data to HDFS by sending it to the NameNode. The NameNode splits the data into blocks and assigns them to DataNodes for storage.\n","\n","2. Data Storage: DataNodes store the blocks and replicate them across multiple nodes for redundancy and fault tolerance.\n","\n","3. Data Retrieval: Clients read data from HDFS by sending a request to the NameNode. The NameNode directs the client to the DataNode that stores the requested block.\n","\n","Data Reliability and Fault Tolerance\n","\n","HDFS provides data reliability and fault tolerance through:\n","\n","1. Replication: Data is replicated across multiple DataNodes to ensure that it is available even in the event of node failure.\n","2. Checksums: HDFS uses checksums to detect data corruption and ensure data integrity.\n","\n","3. Heartbeats: DataNodes send heartbeats to the NameNode to indicate their availability. If a DataNode fails to send a heartbeat, the NameNode assumes it has failed and replicates its data to another node.\n","\n"],"metadata":{"id":"yiZ6W1W2W8Fg"}},{"cell_type":"markdown","source":["**Q.3 Write a step-by-step explanation of how the MapReduce framework works. Use a real-world example to\n","illustrate the Map and Reduce phases. Discuss the advantages and limitations of MapReduce for processing\n","large datasets.**"],"metadata":{"id":"ucJkGlzTXmP0"}},{"cell_type":"markdown","source":["Here's a concise step-by-step explanation of the MapReduce framework:\n","\n","Step 1: Data Preparation\n","Split data into smaller chunks (input splits) for parallel processing.\n","\n","Step 2: Map Phase\n","Mapper tasks process input splits, applying a mapping function to produce key-value pairs.\n","\n","Step 3: Shuffle and Sort Phase\n","Key-value pairs are shuffled and sorted by key for processing in the Reduce phase.\n","\n","Step 4: Reduce Phase\n","Reducer tasks process sorted key-value pairs, applying a reducing function to produce final output.\n","\n","Step 5: Output\n","Final output is written to a file or database.\n","\n","Real-World Example: Calculating total sales for each region.\n","Advantages:\n","\n","1. Scalability\n","2. Flexibility\n","3. Fault tolerance\n","\n","Limitations:\n","\n","1. Complexity\n","2. Performance\n","3. Limited real-time processing"],"metadata":{"id":"1Kdn4Kq2Xy_4"}},{"cell_type":"markdown","source":["**4. Explore the role of YARN in Hadoop. Explain how it manages cluster resources and schedules applications.\n","Compare YARN with the earlier Hadoop 1.x architecture and highlight the benefits of YARN.**"],"metadata":{"id":"Co9a4mkgZaVc"}},{"cell_type":"markdown","source":["Role of YARN:\n","YARN (Yet Another Resource Negotiator) is a resource management layer in Hadoop that manages cluster resources and schedules applications.\n","\n","Key Components:\n","\n","1. ResourceManager (RM): manages cluster resources\n","2. ApplicationMaster (AM): manages application execution\n","3. NodeManager (NM): manages resources on individual nodes\n","\n","How YARN Works:\n","1. Client submits application to RM\n","2. RM allocates resources and launches AM\n","3. AM negotiates resources with NM and executes application\n","4. NM monitors resource usage and reports to RM\n","\n","Comparison with Hadoop 1.x:\n","YARN replaces the traditional MapReduce framework in Hadoop 1.x, offering:\n","\n","1. Improved scalability\n","2. Better resource utilization\n","3. Support for multiple data processing frameworks (e.g., MapReduce, Spark, Flink)\n","\n","Benefits of YARN:\n","1. Increased flexibility\n","2. Improved resource management\n","3. Enhanced scalability and performance"],"metadata":{"id":"BkJAlK9iZIR5"}},{"cell_type":"markdown","source":["**Q 5. Provide an overview of some popular components within the Hadoop ecosystem, such as HBase, Hive, Pig,\n","and Spark. Describe the use cases and differences between these components. Choose one component and\n","explain how it can be integrated into a Hadoop ecosystem for specific data processing tasks**"],"metadata":{"id":"-kkVGpKyZ6R1"}},{"cell_type":"markdown","source":["Here's a concise overview of popular Hadoop ecosystem components:\n","\n","HBase: NoSQL database for real-time data processing.\n","Hive: Data warehousing and SQL-like query language for data analysis.\n","Pig: High-level data processing language and framework for data transformation.\n","Spark: In-memory data processing engine for fast data processing.\n","\n","Use Cases:\n","\n","HBase: Real-time analytics, IoT data processing.\n","Hive: Data warehousing, business intelligence.\n","Pig: Data transformation, data integration.\n","Spark: Real-time analytics, machine learning.\n","\n","Differences:\n","HBase: NoSQL database, real-time data processing.\n","Hive: SQL-like query language, data warehousing.\n","Pig: High-level data processing language, data transformation.\n","Spark: In-memory data processing engine, fast data processing.\n","\n","Integration Example:\n","\n","Let's integrate Spark into a Hadoop ecosystem for real-time data processing tasks:\n","\n","1. Data ingestion: Use Flume or Kafka to ingest data into HDFS.\n","2. Data processing: Use Spark to process data in real-time, leveraging its in-memory processing capabilities.\n","3. Data storage: Store processed data in HBase or Hive for further analysis.\n","4. Data analytics: Use Spark's machine learning library (MLlib) for predictive analytics."],"metadata":{"id":"TZ_bc2XdaLHz"}},{"cell_type":"markdown","source":["**6. Explain the key differences between Apache Spark and Hadoop MapReduce. How does Spark overcome\n","some of the limitations of MapReduce for big data processing tasks?**"],"metadata":{"id":"P-LqtyUVau7S"}},{"cell_type":"markdown","source":["Here are the key differences between Apache Spark and Hadoop MapReduce:\n","\n","Key Differences:\n","\n","1. Processing Model: MapReduce (batch) vs. Spark (in-memory)\n","2. Processing Speed: Spark is significantly faster\n","3. Data Resilience: Spark provides stronger data resilience\n","4. Programming Model: Spark provides a higher-level programming model\n","\n","How Spark Overcomes MapReduce Limitations:\n","\n","1. In-memory processing for faster speeds\n","2. Interactive analytics and real-time processing\n","3. Simplified programming model for easier development and debugging"],"metadata":{"id":"26zim-QLcjoo"}},{"cell_type":"markdown","source":["Here's a Spark application in Python that reads a text file, counts the occurrences of each word, and returns the top 10 most frequent words:\n","\n","\n"],"metadata":{"id":"OA2JkTZPc0sv"}},{"cell_type":"code","source":["from pyspark import SparkContext\n","\n","# Create SparkContext\n","sc = SparkContext(\"local\", \"Word Count\")\n","\n","# Read text file\n","text = sc.textFile(\"data.txt\")\n","\n","# Split text into words\n","words = text.flatMap(lambda line: line.split())\n","\n","# Count word occurrences\n","word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n","# Get top 10 most frequent words\n","top_10_words = word_counts.takeOrdered(10, key=lambda x: x[1])\n","\n","# Print top 10 most frequent words\n","for word, count in top_10_words:\n","    print(f\"{word}: {count}\")\n"],"metadata":{"id":"CN-uPVVzdScg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Q 8. Using Spark RDDs (Resilient Distributed Datasets), perform the following tasks on a dataset of your\n","choice:\n","a. Filter the data to select only rows that meet specific criteria.\n","b. Map a transformation to modify a specific column in the dataset.\n","c. Reduce the dataset to calculate a meaningful aggregation (e.g., sum, average).**"],"metadata":{"id":"G80jFP2Jeb9S"}},{"cell_type":"code","source":["from pyspark import SparkContext\n","\n","# Create SparkContext\n","sc = SparkContext(\"local\", \"Exam Scores\")\n","\n","# Create a sample dataset of exam scores\n","data = [(\"John\", 85), (\"Mary\", 90), (\"David\", 78), (\"Emily\", 92), (\"James\", 88)]\n","\n","# Create an RDD from the dataset\n","rdd = sc.parallelize(data)\n","\n","# Task a: Filter the data to select only rows where the score is above 85\n","filtered_rdd = rdd.filter(lambda x: x[1] > 85)\n","print(\"Filtered RDD:\", filtered_rdd.collect())\n","\n","# Task b: Map a transformation to modify the score column to add 5 points\n","mapped_rdd = filtered_rdd.map(lambda x: (x[0], x[1] + 5))\n","print(\"Mapped RDD:\", mapped_rdd.collect())\n","\n","# Task b: Map a transformation to modify the score column to add 5 points\n","mapped_rdd = filtered_rdd.map(lambda x: (x[0], x[1] + 5))\n","print(\"Mapped RDD:\", mapped_rdd.collect())\n","\n","# Task c: Reduce the dataset to calculate the average score\n","average_score = mapped_rdd.map(lambda x: x[1]).reduce(lambda a, b: a + b) / mapped_rdd.count()\n","print(\"Average Score:\", average_score)\n","\n","#This example demonstrates the following tasks:\n","#- Filtering the data to select only rows that meet specific criteria (score above 85)\n","#- Mapping a transformation to modify a specific column in the dataset (adding 5 points to the score)\n","#- Reducing the dataset to calculate a meaningful aggregation (average score)#\n"],"metadata":{"id":"QP9f8JQUgM7Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"gLVjnhGNhaDo"}}]}